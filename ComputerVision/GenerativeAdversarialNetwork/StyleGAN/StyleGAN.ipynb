{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StyleGAN from scratch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "https://github.com/SiskonEmilia/StyleGAN-PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torchvision import datasets, utils\n",
    "from torch.utils.data import Dataset, DataLoader, dataloader\n",
    "from torch.multiprocessing import reductions\n",
    "from multiprocessing.reduction import ForkingPickler\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_collate_fun = dataloader.default_collate\n",
    "\n",
    "def default_collate_fun_override(batch):\n",
    "    dataloader._use_share_memory = False\n",
    "    return default_collate_fun(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "setattr(dataloader, 'default_collate', default_collate_fun_override)\n",
    "\n",
    "for t in torch._storage_classes:\n",
    "    if t in ForkingPickler._extra_reducers:\n",
    "        del ForkingPickler._extra_reducers[t]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale weights with HE normal initialization.\n",
    "\n",
    "class ScaleW:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    def scale(self, module):\n",
    "        weight = getattr(module, self.name + '_orig')\n",
    "        \n",
    "        # Calculate the number of parameters in previous layer.\n",
    "        # (number of input channels) * (number of parameters in one channel) \n",
    "        n_in = weight.data.size(1) * weight.data[0][0].numel()\n",
    "\n",
    "        return weight * math.sqrt(2. / n_in)\n",
    "    \n",
    "    @staticmethod\n",
    "    def apply(module, name):\n",
    "        hook = ScaleW(name)\n",
    "        weight = getattr(module, name)\n",
    "        module.register_parameter(name + '_orig', nn.Parameter(weight.data))\n",
    "        del module._parameters[name]\n",
    "        module.register_forward_pre_hook(hook)\n",
    "\n",
    "    def __call__(self, module):\n",
    "        weight = self.scale(module)\n",
    "        setattr(module, self.name, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick apply for scaled weight\n",
    "\n",
    "def quick_scale(module, name='weight'):\n",
    "    ScaleW.apply(module, name)\n",
    "    return module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledLinear(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super().__init__()\n",
    "\n",
    "        linear = nn.Linear(dim_in, dim_out)\n",
    "        # Initialize weight with normal distribution.\n",
    "        linear.weight.data.normal_()\n",
    "        # Initialize bias to zero.\n",
    "        linear.bias.data.zero_()\n",
    "        \n",
    "        self.linear = quick_scale(linear)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledConv2d(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        conv = nn.Conv2d(*args, **kwargs)\n",
    "        conv.weight.data.normal_()\n",
    "        conv.bias.data.zero_()\n",
    "\n",
    "        self.conv = quick_scale(conv)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize channels of an input image. \n",
    "class PixelNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Consider mean of x as 0.\n",
    "        # x_norm = x - mu / sigma\n",
    "        sigma = torch.sqrt(torch.mean(x**2, dim=1, keep_dim=True) + 1e-8)\n",
    "        return x / sigma"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learned Affine Transform `A`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Flcwl8%2FbtrZtqZJohK%2F2MdeTwsRpXSazVdKdzHB1K%2Fimg.png\"></img>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FDDNSS%2FbtrZD2cAUXE%2FN7OUemuCGUwCgVMssVw1yk%2Fimg.jpg\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply affine transformation A to w.\n",
    "# Input: w\n",
    "# Shape of w: (n_channel)\n",
    "# Output: style = [y_si, y_bi]\n",
    "# Shape of style: (2 * n_channel)\n",
    "\n",
    "class LearnedA(nn.Module):\n",
    "    def __init__(self, dim_latent, n_channel):\n",
    "        super().__init__()\n",
    "        self.transform = ScaledLinear(dim_latent, n_channel * 2)\n",
    "        self.transform.linear.bias.data[:n_channel] = 1\n",
    "        self.transform.linear.bias.data[n_channel:] = 0\n",
    "\n",
    "    def forward(self, w):\n",
    "        style = self.transform(w).unsqueeze(2).unsqueeze(3)\n",
    "        return style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaIN(nn.Module):\n",
    "    def __init__(self, n_channel):\n",
    "        super().__init__()\n",
    "        self.norm = nn.InstanceNorm2d(n_channel)\n",
    "    \n",
    "    def forward(self, image, style):\n",
    "        # Split style tensor to two parts along channel dimension.\n",
    "        scaling, bias = style.chunk(2, dim=1)\n",
    "        image_norm = self.norm(image)\n",
    "        result = scaling * image_norm + bias\n",
    "        return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Noise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FdQh8aw%2FbtrZD2RepLK%2FRvLKDB7gvR9y2QCYCmuLNK%2Fimg.jpg\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnedB(nn.Module):\n",
    "    def __init__(self, n_channel):\n",
    "        super().__init__()\n",
    "        # Broadcasting is automatically applied, so it will have the same dimension as an input.\n",
    "        self.weight = nn.Parameter(torch.zeros(1, n_channel, 1, 1))\n",
    "    \n",
    "    def forward(self, noise):\n",
    "        return noise * self.weight"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthesis network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FdjieWG%2FbtrZAtCfadE%2F1LvbQdbBYdRMe4qQ6qGzM0%2Fimg.jpg\"></img>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Convolution Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FirstSynthesisBlock(nn.Module):\n",
    "    def __init__(self, n_channel, dim_latent, dim_input):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.constant = nn.Parameter(torch.randn(1, n_channel, dim_input, dim_input))\n",
    "        \n",
    "        self.style1 = LearnedA(dim_latent, n_channel)\n",
    "        self.style2 = LearnedA(dim_latent, n_channel)\n",
    "\n",
    "        self.noise1 = quick_scale(LearnedB(n_channel))\n",
    "        self.noise2 = quick_scale(LearnedB(n_channel))\n",
    "\n",
    "        self.ada_in = AdaIN(n_channel)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
    "        self.conv = ScaledConv2d(n_channel, n_channel, kernel=3, padding=1)\n",
    "    \n",
    "    def forward(self, w, noise):\n",
    "        # Constant is used as a seed for generating result.\n",
    "        \n",
    "        # Step 1\n",
    "        seed = self.constant.repeat(noise.shape[0], 1, 1, 1)\n",
    "        result = seed + self.noise1(noise)\n",
    "        style1 = self.style1(w)\n",
    "        result = self.ada_in(result, style1)\n",
    "        result = self.leaky_relu(result)\n",
    "        \n",
    "        \n",
    "        # Step 2\n",
    "        result = self.conv(result)\n",
    "        result = result + self.noise2(noise)\n",
    "        style2 = self.style2(w)\n",
    "        result = self.ada_in(result, style2)\n",
    "        result = self.leaky_relu(result)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SynthesisBlock(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, dim_latent):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.style1 = LearnedA(dim_latent, out_channel)\n",
    "        self.style2 = LearnedA(dim_latent, out_channel)\n",
    "\n",
    "        self.noise1 = quick_scale(LearnedB(out_channel))\n",
    "        self.noise2 = quick_scale(LearnedB(out_channel))\n",
    "\n",
    "        self.ada_in = AdaIN(out_channel)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
    "        self.conv1 = ScaledConv2d(in_channel, out_channel, kernel=3, padding=1)\n",
    "        self.conv2 = ScaledConv2d(out_channel, out_channel, kernel=3, padding=1)\n",
    "    \n",
    "    def forward(self, input, w, noise):\n",
    "        # An input is upsampled twice from the previous one by interpolation.\n",
    "        \n",
    "        # Step 1\n",
    "        result = self.conv1(input)\n",
    "        result = result + self.noise1(noise)\n",
    "        style1 = self.style1(w)\n",
    "        result = self.ada_in(result, style1)\n",
    "        result = self.leaky_relu(result)\n",
    "   \n",
    "        \n",
    "        # Step 2\n",
    "        result = self.conv2(result)\n",
    "        result = result + self.noise2(noise)\n",
    "        style2 = self.style2(w)\n",
    "        result = self.ada_in(result, style2)\n",
    "        result = self.leaky_relu(result)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbxAGqg%2FbtrZCpTqDQo%2FETgEG2hnVdrbk8LyBmjLXK%2Fimg.jpg\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MappingNetwork(nn.Module):\n",
    "    def __init__(self, n_fc, dim_latent):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = [PixelNorm()]\n",
    "\n",
    "        for _ in range(n_fc):\n",
    "            layers.append(ScaledLinear(dim_latent, dim_latent))\n",
    "            layers.append(nn.LeakyReLU(0.2))\n",
    "        \n",
    "        self.mapping = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, z):\n",
    "        w = self.mapping(z)\n",
    "        return w"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, n_fc, dim_latent, dim_input):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mapping_net = MappingNetwork(n_fc)\n",
    "\n",
    "        in_channels = [512, 512, 512, 512, 256, 128, 64, 32]\n",
    "        out_channels = [512, 512, 512, 256, 128, 64, 32, 16]\n",
    "\n",
    "        # 9 synthesis block(1 + 8)\n",
    "        self.convs = nn.ModuleList([\n",
    "            FirstSynthesisBlock(512, dim_latent, dim_input),\n",
    "            *(SynthesisBlock(in_channels[idx], out_channels[idx], dim_latent) for idx in range(len(in_channels)))\n",
    "        ])\n",
    "\n",
    "        self.to_rgbs = nn.ModuleList([\n",
    "            ScaledConv2d(out_channels[idx], 3, kernel_size=1) for idx in range(len(in_channels))\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, z, step, alpha, noise=None, mix_steps=[]):\n",
    "        if type(z) != type(list()):\n",
    "            z = [z]\n",
    "        \n",
    "        w = [self.mapping_net(latent_z) for latent_z in z]\n",
    "        batch_size = w[0].size(0)\n",
    "\n",
    "        result = 0\n",
    "        current_latent = 0\n",
    "        \n",
    "        for i, conv in enumerate(self.convs):\n",
    "            if i in mix_steps:\n",
    "                current_latent = w[1]\n",
    "            else:\n",
    "                current_latent = w[0]\n",
    "\n",
    "            # Except the first layer\n",
    "            if i > 0 and step > 0:\n",
    "                # Upsample the previous output\n",
    "                result_upsampled = nn.functional.interpolate(result, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "                result = conv(result_upsampled, current_latent, noise[i])\n",
    "            else:\n",
    "                result = conv(current_latent, noise[i])\n",
    "\n",
    "            # Last layer\n",
    "            if i == step:\n",
    "                result = self.to_rgbs[i](result)\n",
    "\n",
    "                if i > 0 and 0 <= alpha < 1:\n",
    "                    result_prev = self.to_rgbs[i - 1](result_upsampled)            \n",
    "                    result = alpha * result + (1 - alpha(result_prev))\n",
    "\n",
    "                break\n",
    "\n",
    "        return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorConvBlock(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, size_kernel1, padding1, size_kernel2=None, padding2=None):\n",
    "        super().__init__()\n",
    "\n",
    "        if size_kernel2 == None:\n",
    "            size_kernel2 = size_kernel1\n",
    "        \n",
    "        if padding2 == None:\n",
    "            padding2 = padding1\n",
    "\n",
    "        self.convs = nn.Sequential(\n",
    "            ScaledConv2d(in_channel, out_channel, size_kernel1, padding=padding1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            ScaledConv2d(out_channel, out_channel, size_kernel2, padding=padding2),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "    def forward(self, image):\n",
    "        result = self.conv(image)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        in_channels = [512, 512, 512, 256, 128, 64, 32, 16]\n",
    "        out_channels = [512, 512, 512, 512, 256, 128, 64, 32]\n",
    "\n",
    "        in_channels.reverse()\n",
    "        out_channels.reverse()\n",
    "\n",
    "        self.from_rgbs = nn.ModuleList([\n",
    "            ScaledConv2d(3, in_channels[idx], kernel_size=1) for idx in range(len(in_channels))\n",
    "        ] + [ScaledConv2d(3, 512, 1)])\n",
    "\n",
    "        self.convs = nn.ModuleList([\n",
    "            DiscriminatorConvBlock(in_channels[idx], out_channels[idx], size_kernel1=3, padding1=1) for idx in range(len(in_channels))\n",
    "        ] + [DiscriminatorConvBlock(512 + 1, 512, 3, 1, 4, 0)])\n",
    "\n",
    "        self.fc = ScaledLinear(512, 1)\n",
    "        \n",
    "        self.n_layer = 9\n",
    "\n",
    "    def forward(self, image, step=0, alpha=-1):\n",
    "        for i in range(step):\n",
    "            if i == 0:\n",
    "                result = self.from_rgbs[i](image)\n",
    "            \n",
    "            if i == step:\n",
    "                # shape of result: (batch, channel=512, 4, 4)\n",
    "                res_var = result.var(dim=0, unbiased=True) + 1e-8\n",
    "                # shape of res_var: (512, 4, 4)\n",
    "                res_std = torch.sqrt(res_var)\n",
    "                # shape of re_std: (512, 4, 4)\n",
    "                mean_std = res_std.mean().expand(result.size(0), 1, 4, 4)\n",
    "                # shape of mean_std: (1) -> (batch, 1, 4, 4)\n",
    "                result = torch.cat([result, mean_std], dim=1)\n",
    "                # shape of result: (batch, 512 + 1, 4, 4)\n",
    "\n",
    "            result = self.convs[i](result)\n",
    "            # shape of result: (batch, 512, 4, 4)\n",
    "\n",
    "            if i < step:\n",
    "                result = nn.functional.interpolate(result, scale_factor=0.5, mode='bilinear', align_corners=False)\n",
    "\n",
    "                if i == 0 and 0 <= alpha < 1:\n",
    "                    result_next = self.from_rgbs[i + 1](image)\n",
    "                    result_next = nn.functional.interpolate(result_next, scale_factor=0.5, mode='bilinear', align_corners=False)\n",
    "\n",
    "                    result = alpha * result + (1 - alpha) * result_next\n",
    "\n",
    "        # shape of result: (batch, 512, 4, 4) => (batch, 512)\n",
    "        result = result.squeeze(2).squeeze(2)\n",
    "        result = self.fc(result)\n",
    "        return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use idel gpu\n",
    "# it's better to use enviroment variable\n",
    "# if you want to use multiple gpus, please\n",
    "# modify hyperparameters at the same time\n",
    "# And Make Sure Your Pytorch Version >= 1.0.1\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1, 2'\n",
    "n_gpu             = 2\n",
    "device            = torch.device('cuda:0')\n",
    "\n",
    "learning_rate     = {128: 0.0015, 256: 0.002, 512: 0.003, 1024: 0.003}\n",
    "batch_size_1gpu   = {4: 128, 8: 128, 16: 64, 32: 32, 64: 16, 128: 16}\n",
    "mini_batch_size_1 = 8\n",
    "batch_size        = {4: 256, 8: 256, 16: 128, 32: 64, 64: 32, 128: 16}\n",
    "mini_batch_size   = 8\n",
    "batch_size_4gpus  = {4: 512, 8: 256, 16: 128, 32: 64, 64: 32}\n",
    "mini_batch_size_4 = 16\n",
    "batch_size_8gpus  = {4: 512, 8: 256, 16: 128, 32: 64}\n",
    "mini_batch_size_8 = 32\n",
    "n_fc              = 8\n",
    "dim_latent        = 512\n",
    "dim_input         = 4\n",
    "n_sample          = 120000\n",
    "DGR               = 1\n",
    "n_show_loss       = 40\n",
    "step              = 1 # Train from (8 * 8)\n",
    "max_step          = 8 # Maximum step (8 for 1024^2)\n",
    "style_mixing      = [] # Waiting to implement\n",
    "image_folder_path = './dataset/'\n",
    "save_folder_path  = './results/'\n",
    "\n",
    "low_steps         = [0, 1, 2]\n",
    "# style_mixing    += low_steps\n",
    "mid_steps         = [3, 4, 5]\n",
    "# style_mixing    += mid_steps\n",
    "hig_steps         = [6, 7, 8]\n",
    "# style_mixing    += hig_steps\n",
    "\n",
    "# Used to continue training from last checkpoint\n",
    "startpoint        = 0\n",
    "used_sample       = 0\n",
    "alpha             = 0\n",
    "\n",
    "# Mode: Evaluate? Train?\n",
    "is_train          = True\n",
    "\n",
    "# How to start training?\n",
    "# True for start from saved model\n",
    "# False for retrain from the very beginning\n",
    "is_continue       = True\n",
    "d_losses          = [float('inf')]\n",
    "g_losses          = [float('inf')]\n",
    "\n",
    "inputs, outputs = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameter whether it is required for gradient descent or not.\n",
    "def set_grad_flag(module, flag):\n",
    "    for p in module.parameters():\n",
    "        p.requires_grad = flag\n",
    "\n",
    "# Reset the Learning rate.\n",
    "def reset_LR(optimizer, lr):\n",
    "    for pam_group in optimizer.param_groups:\n",
    "        mul = pam_group.get('mul', 1)\n",
    "        pam_group['lr'] = lr * mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gain sample with DataLoader\n",
    "def gain_sample(dataset, batch_size, image_size=4):\n",
    "    transform = transforms.Compose([\n",
    "            transforms.Resize(image_size),          # Resize to the same size\n",
    "            transforms.CenterCrop(image_size),      # Crop to get square area\n",
    "            transforms.RandomHorizontalFlip(),      # Increase number of samples\n",
    "            transforms.ToTensor(),            \n",
    "            transforms.Normalize((0.5, 0.5, 0.5),   # Normalize RGB values\n",
    "                                 (0.5, 0.5, 0.5))])\n",
    "\n",
    "    dataset.transform = transform\n",
    "    loader = DataLoader(dataset, shuffle=True, batch_size=batch_size, num_workers=8)\n",
    "\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(tensor, i):\n",
    "    grid = tensor[0]\n",
    "    grid.clamp_(-1, 1).add_(1).div_(2)\n",
    "    # Add 0.5 after unnormalizing to [0, 255] to round to nearest integer\n",
    "    ndarr = grid.mul_(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).to('cpu', torch.uint8).numpy()\n",
    "    img = Image.fromarray(ndarr)\n",
    "    img.save(f'{save_folder_path}sample-iter{i}.png')\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(generator, discriminator, g_optim, d_optim ,dataset , step, start_point=0, used_sample=0, d_losses=[], g_losses=[], alpha=0):\n",
    "    resolution = 4 * 2 ** step\n",
    "\n",
    "    origin_loader = gain_sample(dataset, batch_size.get(resolution, mini_batch_size), resolution)\n",
    "    data_loader = iter(origin_loader)\n",
    "\n",
    "    reset_LR(g_optim, learning_rate.get(resolution, 0.001))\n",
    "    reset_LR(d_optim, learning_rate.get(resolution, 0.001))\n",
    "\n",
    "    progress_bar = tqdm(range(start_point + 1, n_sample * 5))\n",
    "\n",
    "    for i in progress_bar:\n",
    "        alpha = min(1, alpha + batch_size.get(resolution, mini_batch_size) / (n_sample * 2))\n",
    "        \n",
    "        if used_sample > n_sample * 2 and step < max_step:\n",
    "            step += 1\n",
    "\n",
    "            alpha = 0\n",
    "            used_sample = 0\n",
    "            \n",
    "            resolution = 4 * 2 ** step\n",
    "\n",
    "            # Change batch size.\n",
    "\n",
    "            del origin_loader\n",
    "            del data_loader\n",
    "\n",
    "            origin_loader = gain_sample(dataset, batch_size.get(resolution, mini_batch_size), resolution)\n",
    "            data_loader = iter(origin_loader)\n",
    "\n",
    "            reset_LR(g_optim, learning_rate.get(resolution, 0.001))\n",
    "            reset_LR(d_optim, learning_rate.get(resolution, 0.001))\n",
    "\n",
    "\n",
    "        try:\n",
    "            # Try to read next image\n",
    "            real_image, label = next(data_loader)\n",
    "\n",
    "        except (OSError, StopIteration):\n",
    "            # Dataset exhausted, train from the first image\n",
    "            data_loader = iter(origin_loader)\n",
    "            real_image, label = next(data_loader)\n",
    "\n",
    "        used_sample += real_image.shape[0]\n",
    "\n",
    "        real_image = real_image.to(device)\n",
    "\n",
    "\n",
    "        # Train Discriminator first.\n",
    "        # Set all gradients of discriminator to zero.\n",
    "        discriminator.zero_grad()\n",
    "        set_grad_flag(discriminator, True)\n",
    "        set_grad_flag(generator, False)\n",
    "\n",
    "        real_image.required_grad = True\n",
    "        if n_gpu > 1:\n",
    "            real_predict = nn.parallel.data_parallel(discriminator,(real_image, step, alpha), range(n_gpu))\n",
    "        else:\n",
    "            real_predict = discriminator(real_image, step, alpha)\n",
    "\n",
    "        # Calculate the loss of prediction for real image.\n",
    "        # Discriminator should predict 1 to the real image.\n",
    "        real_predict = nn.functional.softplus(-real_predict).mean()\n",
    "        real_predict.backward()\n",
    "\n",
    "        grad_real = torch.autograd.grad(outputs=real_predict.sum(), inputs=real_image, create_graph=True)[0]\n",
    "        grad_penalty_real = (grad_real.view(grad_real.size(0), -1).norm(2, dim=1) ** 2).mean()\n",
    "        grad_penalty_real = 10 / 2 * grad_penalty_real\n",
    "        grad_penalty_real.backward()\n",
    "\n",
    "        # Generate latent code\n",
    "        w1 = [torch.randn((batch_size.get(resolution, mini_batch_size), dim_latent), device=device)]\n",
    "        w2 = [torch.randn((batch_size.get(resolution, mini_batch_size), dim_latent), device=device)]\n",
    "\n",
    "        noise_1 = []\n",
    "        noise_2 = []\n",
    "\n",
    "        for m in range(step + 1):\n",
    "            size = 4 * 2 ** m # Due to the upsampling, size of noise will grow\n",
    "            noise_1.append(torch.randn((batch_size.get(resolution, mini_batch_size), 1, size, size), device=device))\n",
    "            noise_2.append(torch.randn((batch_size.get(resolution, mini_batch_size), 1, size, size), device=device))\n",
    "        \n",
    "        if n_gpu > 1:\n",
    "            fake_image = nn.parallel.data_parallel(generator, (w1, step, alpha, noise_1), range(n_gpu))\n",
    "            fake_predict = nn.parallel.data_parallel(discriminator, (fake_image, step, alpha), range(n_gpu))\n",
    "        else:\n",
    "            fake_image = generator(w1, step, alpha, noise_1)\n",
    "            fake_predict = discriminator(fake_image, step, alpha)\n",
    "\n",
    "        # Calculate the loss of prediction for fake image.\n",
    "        # Discriminator sould predict 0 to the fake image. \n",
    "        fake_predict = nn.functional.softplus(fake_predict).mean()\n",
    "        fake_predict.backward()\n",
    "\n",
    "        if i % n_show_loss == 0:\n",
    "            d_losses.append((real_predict + fake_predict).item())\n",
    "        \n",
    "        d_optim.step()\n",
    "\n",
    "        del grad_penalty_real, grad_real, fake_predict, real_predict, w1\n",
    "        \n",
    "        # Train Generator\n",
    "        # Set all gradients of generator to zero.\n",
    "        generator.zero_grad()\n",
    "        set_grad_flag(discriminator, False)\n",
    "        set_grad_flag(generator, True)\n",
    "\n",
    "        if n_gpu > 1:\n",
    "            fake_image = nn.parallel.data_parallel(generator, (w2, step, alpha, noise_2), range(n_gpu))\n",
    "            fake_predict = nn.parallel.data_parallel(discriminator, (fake_image, step, alpha), range(n_gpu))\n",
    "        else:\n",
    "            fake_image = generator(w2, step, alpha, noise_2)\n",
    "            fake_predict = discriminator(fake_image, step, alpha)\n",
    "\n",
    "        fake_predict = nn.functional.softplus(fake_predict).mean()\n",
    "        fake_predict.backward()\n",
    "        g_optim.step()\n",
    "\n",
    "\n",
    "        if i % n_show_loss == 0:\n",
    "            g_losses.append(fake_predict.item())\n",
    "            imshow(fake_image.data.cpu(), i)\n",
    "\n",
    "        del fake_predict, fake_image, w2\n",
    "        \n",
    "        if (i + 1) % 1000 == 0:\n",
    "            # Save the model every 1000 iterations\n",
    "            torch.save({\n",
    "                'generator'    : generator.state_dict(),\n",
    "                'discriminator': discriminator.state_dict(),\n",
    "                'g_optim'      : g_optim.state_dict(),\n",
    "                'd_optim'      : d_optim.state_dict(),\n",
    "                'parameters'   : (step, i, used_sample, alpha),\n",
    "                'd_losses'     : d_losses,\n",
    "                'g_losses'     : g_losses\n",
    "            }, 'checkpoint/trained.pth')\n",
    "            print(f'Iteration {i} successfully saved.')\n",
    "        \n",
    "        progress_bar.set_description((f'Resolution: {resolution}*{resolution}  D_Loss: {d_losses[-1]:.4f}  G_Loss: {g_losses[-1]:.4f}  Alpha: {alpha:.4f}'))\n",
    "    \n",
    "    return d_losses, g_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(n_fc, dim_latent, dim_input).to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "g_optim = optim.Adam([{\n",
    "    'params': generator.convs.parameters(),\n",
    "    'lr': 0.001    \n",
    "}, {\n",
    "    'params': generator.to_rgbs.parameters(),\n",
    "    'lr': 0.001\n",
    "}], lr=0.001, betas=(0.0, 0.99))\n",
    "\n",
    "g_optim.add_param_group({\n",
    "    'params': generator.fcs.parameters(),\n",
    "    'lr': 0.001 * 0.01,\n",
    "    'mul': 0.01\n",
    "})\n",
    "\n",
    "d_optim = optim.Adam(discriminator.parameters(), lr=0.001, betas=(0.0, 0.99))\n",
    "dataset = datasets.ImageFolder(image_folder_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
